---
title: "Modeling | Swire Coca-Cola Capstone Project "
subtitle: "IS 6813-001, Spring 2025 | Group 3"
author:
  - Adam Bushman
  - Georgia Christodoulou
  - Tyler Swanson
  - Zac Mendenhall
date: "3/18/2025"
output:
    html_document:
        theme: simplex
        self_contained: true
        embed-resources: true
toc: true
---

![](swire-banner.png)

<br>

## Business Problem Statement

Regional beverage bottler Swire Coca-Cola (SCCU) relies on two business models: 1) “Red Truck”, which features high-volume customers serviced personally by Swire, and 2) “White Truck” or “Alternate Route to Market”, which entails smaller customers serviced by a third-party distributor.

Swire’s current segmenting strategy has led to misallocation of resources, inflated expenses, and missed opportunities from clients with high-growth potential. Swire aims to better algin their customers with the business proposition of these models by identifying customer characteristics and  rdering behavior that better determines the right business model for the long-term relationship.


## Modeling  Introduction

Lorem ipsum...


## Preparation

### Libraries & Data

We begin by setting up our session with the necessary libraries and data provided by Swire. These will be referenced often throughout this document.

```{r warning = FALSE, message = FALSE}


# Load libraries for EDA
library('tidyverse')  # Data wrangling & visualization
library('gt')         # Create professional tables
library('janitor')    # Clean column names & messy data
library('psych')      # Descriptive stats & psych research tools
library('stringr')    # String manipulation
library('lubridate')  # Handle dates & times
library('rmarkdown')  # Render R Markdown docs
library('dplyr')      # Data manipulation (filter, select, mutate, etc.)
library('skimr')      # Quick summary stats
library('tidyr')      # Reshape/tidy data
library('readxl')     # Read Excel files
library('ggplot2')    # Data visualization
library('readr')      # Read/write CSV & text files
library('knitr')      # Generate dynamic reports
library('leaflet')    # Generate dynamic map
library('geosphere')
library('caret')
library("tidymodels")           # Used for its modeling framework
library("tidyclust")            # Used for clustering approaches
library("kernlab")              # Weighted kernal k-means
```

```{r eval = FALSE}
# Read files into session
transactions <- as.data.frame(data.table::fread("data/transactional_data.csv"))
customer_address <- read.csv("data/customer_address_and_zip_mapping.csv")
customer_profile <- read.csv("data/customer_profile.csv")
delivery_cost <- readxl::read_xlsx('data/delivery_cost_data.xlsx')
```

```{r include = FALSE}

# Branding colors
swire_colors <- list(
  "red" = "#cd0720", 
  "blue" = "#005398", 
  "gray" = "#f2f2f2"
)

# {ggplot2} theme for Swire
theme_swire <- function() {
    theme(
    plot.title.position = "plot", 

    plot.background = element_rect(fill = "white", color = NA), 
    panel.background = element_rect(fill = swire_colors$gray, color = NA), 

    plot.title = element_text(color = swire_colors$red, face = "bold", family = "Poppins"), 
    plot.subtitle = element_text(face = "italic", family = "Poppins"), 
    axis.title = element_text(face = "bold", family = "Poppins"), 
    axis.text = element_text(family = "Poppins"), 

    strip.background = element_rect(fill = swire_colors$blue, color = NA), 
    strip.text = element_text(color = "white", face = "bold")
  )
}
```


## Data Wrangling

Having explored the files individually and making note of important details, we can now move on to wrangling these data into a single object, suitable for the more in-depth analysis upcoming. This wrangling will comprise of:

* Prep individual files for level-of-detail
* Combining the four (4) individual files we collected
* Standardizing column names
* Developing a theory for ordering said columns
* Casting fields to their proper data types
* Derived columns that further describe customer profiles
* Settle on transactional level data with appropriate cost estimates
* Etc.


### Prepping the Cost Data

```{r eval = FALSE}
delivery_cost_expanded <- 
    delivery_cost |>
    # Split the volume range into an object
    mutate(
        range_obj = purrr::map(`Vol Range`, str_split, " - ")
    ) |>
    # Unnest the object for individual reference
    unnest(range_obj) |>
    unnest_wider(range_obj, names_sep = "_") |>
    # Handle the "1350+" scenario
    mutate(
        min_vol = purrr::map_chr(range_obj_1, str_replace, "\\+", ""), 
        max_vol  = ifelse(is.na(range_obj_2), (2^31) - 1, range_obj_2)
    ) |>
    # Turn volumes from charaters to integers
    mutate(
        across(min_vol:max_vol, as.integer)
    ) |>
    # Drop irrelevant columns
    select(-c(range_obj_1, range_obj_2, `Vol Range`))
```

```{r eval = FALSE}
annual_cust_volume <-
    # Take transaction level data
    transactions |>
    # Bring in the customer profile for the `Cold Drink Channel`
    inner_join(
        customer_profile, 
        join_by(CUSTOMER_NUMBER)
    ) |>
    # Get annual cases/gallons by customer
    group_by(YEAR, CUSTOMER_NUMBER, COLD_DRINK_CHANNEL) |>
    summarise(
        annual_cases = sum(DELIVERED_CASES), 
        annual_gallons = sum(DELIVERED_GALLONS), 
        .groups = "drop"
    )
```

```{r eval = FALSE}
delivery_cost_tiers <-
    annual_cust_volume |>
    left_join(
        delivery_cost_expanded |> filter(`Applicable To` != 'Fountain'), 
        join_by(COLD_DRINK_CHANNEL == `Cold Drink Channel`, annual_cases >= min_vol, annual_cases <= max_vol)
    ) |>
    left_join(
        delivery_cost_expanded |> filter(`Applicable To` == 'Fountain'), 
        join_by(COLD_DRINK_CHANNEL == `Cold Drink Channel`, annual_gallons >= min_vol, annual_gallons <= max_vol), 
        suffix = c(".c", ".g")
    ) |>
    select(
        YEAR, CUSTOMER_NUMBER, 
        case_delivery_cost = `Median Delivery Cost.c`, 
        gallon_delivery_cost = `Median Delivery Cost.g`
    )

# Take a peek
head(delivery_cost_tiers)
```


### Prep the Customer Address Object

```{r eval = FALSE}
cust_addr_expanded <-
    customer_address |>
    # Split the full address into an object
    mutate(
        addr_obj = purrr::map(full.address, str_split, ",")
    ) |>
    # Unnest the object for individual reference
    unnest(addr_obj) |>
    unnest_wider(addr_obj, names_sep = "_") |>
    # Pad the zip code with leading zeros and make a character
    mutate(
        zip = str_pad(zip, 5, "left", pad = "0")
    ) |>
    # Rename columns
    rename(
        city = addr_obj_2, 
        state = addr_obj_3, 
        state_abbr = addr_obj_4, 
        county = addr_obj_5, 
        lat = addr_obj_7, 
        lon = addr_obj_8
    ) |>
    # Turn lat/lon values to numeric
    mutate(
        across(lat:lon, as.numeric)
    ) |>
    # Drop irrelevant columns
    select(-c(full.address, addr_obj_1, addr_obj_6))
```


### Combine Individual Files

```{r eval = FALSE}
combined_data_raw <-
    # Take transactions
    transactions |>
    # Join the customer profile data thereto
    inner_join(
        customer_profile |> mutate(zip = str_pad(
            ZIP_CODE, 5, "left", "0"
        )), 
        join_by(CUSTOMER_NUMBER)
    ) |>
    # Join the customer address data thereto
    inner_join(
        cust_addr_expanded, 
        join_by(zip)
    ) |>
    # Join the delivery cost tiers data thereto
    inner_join(
        delivery_cost_tiers, 
        join_by(YEAR, CUSTOMER_NUMBER)
    )
```


### Standardize Names & Data Types

```{r eval = FALSE}
combined_data_std <- 
    # Take the combined data from above
    combined_data_raw |>
    # Standardize the names
    clean_names() |>
    # Standardize data types
    mutate(
        # Convert charater dates to date types
        across(c(transaction_date, first_delivery_date, on_boarding_date), lubridate::mdy), 
        # Turn IDs into characters
        across(c(customer_number, primary_group_number), as.character), 
        # Turn finite categorical fields into factors
        across(
            c(order_type, cold_drink_channel, frequent_order_type, trade_channel, sub_trade_channel, state, state_abbr), 
            as.factor
        )
    ) |>
    # Remove irrelevant columns
    select(-c(zip_code))
```


### Enrich Dataset with New Fields

```{r eval = FALSE}
swire_data_full <-
    combined_data_std |>
    # Add new fields
    mutate(
        # Calculate delivered gallons cost
        # Assume a return is only half as costly as a normal delivery
        delivered_gallons_cost = case_when(
            delivered_gallons < 0 ~ -1 * delivered_gallons * gallon_delivery_cost / 2, 
            TRUE ~ delivered_gallons * gallon_delivery_cost
        ), 
        # Calculate delivered case cost
        # Assume a return is only half as costly as a normal delivery
        delivered_cases_cost = case_when(
            delivered_cases < 0 ~ -1 * delivered_cases * case_delivery_cost / 2, 
            TRUE ~ delivered_cases * case_delivery_cost
        ),
        # Create 'total' columns representing the sum of gallons & cases
        ordered_total = ordered_gallons + ordered_cases, 
        loaded_total = loaded_gallons + loaded_cases, 
        delivered_total = delivered_gallons + delivered_cases, 
    ) |>
    group_by(year, primary_group_number) |>
    mutate(
        # Calculate number of customers belonging to each primary group by year
        primary_group_customers = ifelse(is.na(primary_group_number), 0, n_distinct(customer_number))
    ) |>
    group_by(year, customer_number) |>
    mutate(
        # Calculate how often a customer issues a return each year
        return_frequency = sum(ifelse(delivered_cases < 0 | delivered_gallons < 0, 1, 0))
    ) |>
    ungroup() |>
    # Drop select columns that are no longer relevant
    select(-c(gallon_delivery_cost, case_delivery_cost)) |>
    # Order the columns logically
    select(
        # CUSTOMER PROFILE ITEMS
        customer_number, primary_group_number, primary_group_customers, 
        on_boarding_date, first_delivery_date, cold_drink_channel, frequent_order_type, trade_channel, sub_trade_channel, local_market_partner, co2_customer, city, zip, state, state_abbr, county, lat, lon, 
        
        # TRANSACTION DETAILS
        transaction_date, week, year, order_type, 
        ordered_cases, loaded_cases, delivered_cases, delivered_cases_cost, 
        ordered_gallons, loaded_gallons, delivered_gallons, delivered_gallons_cost, 
        ordered_total, loaded_total, delivered_total, 
        return_frequency
    )
```

### Final Data Set

```{r include = FALSE}

swire_data_full <- readRDS('data/swire_data_full.Rds')
```

```{r}
glimpse(swire_data_full)
```

Now, we have a single, standardized data set that is enriched, properly formatted, and well-suited to the remaining analysis.


## Modeling Approaches

### Approach 1

Lorem ipsum...


### Approach 2

This approach is another take on clustering. The idea is to enrich the data with additional features, most notably leveraging the spatial data we have to understand "neighboring" customers of swire. Here's a simple graphic to illustrate the idea:

![](img/nearest-customers-diagram.png)

For each customer, it may be helpful to know how isolated or densely populated they are. We can approximate this using the "Haversine Distance" formula. In short, it utilizes the latitude and longitude, adjusting for eath's curvature, to approximate the "as the crow flies" distance.

Knowing which 5 customers are closest to each, we can determine order volume, return rates, etc., for these neighbors. **These fields could be helpful in determining the value of an area**; if an area is valuable, even if the customer isn't *yet*, there may be a statistical argument for their potential for red truck.

Let's derive these distances.

#### Customer Level of Detail

We want to get the data to one customer per row. Since we're currently at the level of detail of transactions, we need to do some prep work.

```{r}
swire_cust_grp <-
    swire_data_full |>
    group_by(
        pick(customer_number:lon, return_frequency, year),
    ) |>
    summarise(
        order_transactions = sum(ifelse(ordered_total > 0, 1, 0)), 
        order_transaction_std = sd(ordered_total), 
        delivery_transactions = sum(ifelse(delivered_total > 0, 1, 0)), 
        across(ordered_cases:delivered_total, ~ sum(.x))
    ) |>
    ungroup()
```

```{r}
swire_cust_uniq <- 
    swire_cust_grp |>
    pivot_wider(
        id_cols = c(customer_number:primary_group_number, on_boarding_date:lon), 
        names_from = year, 
        values_from = c(primary_group_customers, return_frequency:delivered_total), 
        names_glue = "{.value}_{year}", 
        values_fill = 0
    )
    
swire_cust_clean <- 
    swire_cust_uniq |>
    replace_na(list(
        order_transaction_std_2023 = mean(swire_cust_uniq$order_transaction_std_2023, na.rm = TRUE), 
        order_transaction_std_2024 = mean(swire_cust_uniq$order_transaction_std_2024, na.rm = TRUE), 
        delivered_gallons_cost_2023 = 0, 
        delivered_gallons_cost_2024 = 0, 
        delivered_cases_cost_2023 = 0,
        delivered_cases_cost_2024 = 0

    )) |>
    select(-c(city, zip, state_abbr, county))
```


#### Distance Calculations

```{r eval = FALSE}
if(file.exists('data/cust_dist.rda')) {
    load('data/cust_dist.rda')
} else {
    kansas.data <- swire_cust_grp |> filter(state == 'Kansas')
    kansas.dist <- distm(kansas.data[,c("lon", "lat")], fun = distHaversine)

    mass.data <- swire_cust_grp |> filter(state == 'Massachusetts')
    mass.dist <- distm(mass.data[,c("lon", "lat")], fun = distHaversine)

    maryland.data <- swire_cust_grp |> filter(state == 'Maryland')
    maryland.dist <- distm(maryland.data[,c("lon", "lat")], fun = distHaversine)

    kentucky.data <- swire_cust_grp |> filter(state == 'Kentucky')
    kentucky.dist <- distm(kentucky.data[,c("lon", "lat")], fun = distHaversine)

    louisiana.data <- swire_cust_grp |> filter(state == 'Louisiana')
    louisiana.dist <- distm(louisiana.data[,c("lon", "lat")], fun = distHaversine)

    cust_dist <- list(
        "louisiana" = list(
            "dist" = louisiana.dist, 
            "cust" = louisiana.data$customer_number, 
            "group" = louisiana.data$primary_group_number
        ), 
        "kansas" = list(
            "dist" = kansas.dist, 
            "cust" = kansas.data$customer_number, 
            "group" = kansas.data$primary_group_number
        ), 
        "mass" = list(
            "dist" = mass.dist, 
            "cust" = mass.data$customer_number, 
            "group" = mass.data$primary_group_number
        ), 
        "maryland" = list(
            "dist" = maryland.dist, 
            "cust" = maryland.data$customer_number, 
            "group" = maryland.data$primary_group_number
        ), 
        "kentucky" = list(
            "dist" = kentucky.dist, 
            "cust" = kentucky.data$customer_number, 
            "group" = kentucky.data$primary_group_number
        )
    )

    save(cust_dist, file = "data/cust_dist.rda")
}
```

```{r eval = FALSE}
diag(cust_dist$louisiana$dist) <- Inf
diag(cust_dist$kansas$dist) <- Inf
diag(cust_dist$mass$dist) <- Inf
diag(cust_dist$maryland$dist) <- Inf
diag(cust_dist$kentucky$dist) <- Inf
```

```{r eval = FALSE}
get_top5 <- function(row) {
    idx <- order(setdiff(row, c(0)))[1:5]
    values <- row[idx] / 1000
    list(indices = idx, values = values)
}
```


```{r eval = FALSE}
dist_df <- data.frame(matrix(NA, nrow = 0, ncol = 4)) |>
    mutate(
        X1 = as.character(X1),
        X2 = as.character(X2),
        X3 = as.integer(X3),
        X4 = as.numeric(X4)
    )
colnames(dist_df) <- c("customer_number", "primary_group_number", "indices", "distances")

for(state in names(cust_dist)) {
    results <- apply(cust_dist[[state]]$dist, 1, get_top5)

    results_tbl <- tibble(
        customer_number = as.character(cust_dist[[state]]$cust), 
        primary_group_number = as.character(cust_dist[[state]]$group), 
        indices = lapply(results, function(x) x$indices),
        distances = lapply(results, function(x) x$values)
    ) |>
        unnest_longer(c("indices", "distances")) |>
        inner_join(
            tibble(
                neighbor_number = as.character(cust_dist[[state]]$cust)
            ) |> rowid_to_column()
            , join_by(indices == rowid)
        )

    dist_df <- bind_rows(dist_df, results_tbl)
}
```


#### Intersecting With Customers

Now that we have generated the "neighbors" for each customer, we can join back to our cleaned customer data and calculate summary values for all of the neighbors.

```{r eval = FALSE}
customer_neighbors <- dist_df |>
    as_tibble() |>
    inner_join(
        swire_cust_clean, 
        join_by(neighbor_number == customer_number)
    ) |>
    mutate(
        same_group_flg = primary_group_number.x == primary_group_number.y
    ) |>
    group_by(
        customer_number
    ) |>
    summarise(
        neighbor_avg_dist_km = mean(distances), 
        neighbor_primary_group_count = sum(same_group_flg, na.rm = TRUE), 
        neighbor_local_market_partners = sum(local_market_partner), 
        neighbor_avg_return_freq = (mean(return_frequency_2023) + mean(return_frequency_2024)) / 2, 
        neighbor_avg_order_transactions_2023 = mean(order_transactions_2023),  
        neighbor_avg_order_transactions_2024 = mean(order_transactions_2024), 
        neighbor_avg_order_transaction_std_2023 = mean(order_transaction_std_2023), 
        neighbor_avg_order_transaction_std_2024 = mean(order_transaction_std_2024), 
        neighbor_avg_ordered_total_2023 = mean(ordered_total_2023), 
        neighbor_avg_ordered_total_2024 = mean(ordered_total_2024)
    ) |>
    ungroup()

customer_neighbors_clean <- 
    customer_neighbors |>
    mutate(neighbor_avg_dist_km = ifelse(
        is.infinite(neighbor_avg_dist_km), 
        max(ifelse(!is.infinite(neighbor_avg_dist_km), neighbor_avg_dist_km, NA), na.rm = TRUE), 
        neighbor_avg_dist_km
    )) |>
    replace_na(list(
        neighbor_avg_dist_km = median(customer_neighbors$neighbor_avg_dist_km, na.rm = TRUE), 
        neighbor_avg_order_transaction_std_2023 = mean(customer_neighbors$neighbor_avg_order_transaction_std_2023, na.rm = TRUE), 
        neighbor_avg_order_transaction_std_2024 = mean(customer_neighbors$neighbor_avg_order_transaction_std_2024, na.rm = TRUE)
    ))
```


#### Enriching Additional Values

We now gave great summary values for the neighbors of each customer. Let's now get our full customer level of detail data together and finalize some data enrichment:

```{r include = FALSE}
customer_neighbors_clean <- as.data.frame(data.table::fread("data/customer_neighbor_fields.csv"))
```

```{r}
swire_cust_enriched <- 
    swire_cust_clean |>
    inner_join(customer_neighbors_clean) |>
    mutate(
        across(c(on_boarding_date, first_delivery_date), as.Date), 
        across(c(customer_number, primary_group_number), as.character), 
        across(c(co2_customer, local_market_partner), as.integer), 
        customer_tenure_yrs = round(as.integer(lubridate::ymd("2024-12-31") - on_boarding_date) / 365.25, 1), 
        ramp_up_mon = round(as.integer(first_delivery_date - on_boarding_date) * 12 / 365.25, 1)
    ) |>
    select(-c(on_boarding_date, first_delivery_date))
```

We now have a really feature rich dataset with additional features describing the area and neighboring customers. 

```{r}
glimpse(swire_cust_enriched)
```


#### Prep for Clustering

We need to turn categorical data into one-hot encoded (dummy) variables so we can run PCA.

```{r}
swire_numeric <- swire_cust_enriched |> select(where(is.numeric))
swire_non_numeric <- swire_cust_enriched |> 
    select(!where(is.numeric)) |>
    select(-c(customer_number, primary_group_number, sub_trade_channel)) |>
    mutate(
        across(everything(), ~factor(make.names(.)))
    )
```

```{r}
swire_dmy <- dummyVars("~ .", swire_non_numeric)
data_dmy <- data.frame(predict(swire_dmy, swire_non_numeric))
```


```{r}
swire_cust_combined <-
    swire_numeric |>
    bind_cols(data_dmy) |>
    select(-c(trade_channel.PHARMACY.RETAILER))
```

Let's now scale these values:

```{r}
swire_scaled <- scale(swire_cust_combined)
```

Let's run PCA and get a sense for how well it does:

```{r}
swire_pca <- prcomp(swire_scaled)
```

```{r}
summary(swire_pca)
```

With only 33 principal components, we're capturing 80% of the variability. This is probably sufficient for us to proceed.

#### Clustering Algorithm Setup

We're going to try two different algorithms: `kmeans` and `hierarchical` clustering. We'll setup two functions that will calculate the within sum of squares (`wss`).

```{r}
cluster_kmeans <- function(k, data) {
    fit <- kmeans(data, k)
    vals <- glance(fit)
    return(vals$tot.withinss)
}

cluster_hclust <- function(k, data) {
    # Run the algorithm
    model <- hier_clust(num_clusters = k, linkage_method = "complete")
    fit <- model |> fit(~., data = as.data.frame(data))

    wss <- fit |>
        sse_within() |>
        select(wss) |>
        unlist() |>
        sum()
    return(wss)
}
```

Let's setup a grid where we can save different values for these algorithms against different cluster sizes (2-8) and WSS values:

```{r}
set.seed(814)

swire_pca_80_sampl <- swire_pca_80 |> sample_n(size = 5000)

clustering_grid <- data.frame(
    clusters = 2:8
)
```

We've also taken a sample of the data for computational efficiency. Let's use a function to plot the outcomes:

```{r}
show_cluster_results <- function(results, title) {
    results |>
        pivot_longer(cols = -clusters) |>
        unnest(value) |>
        ggplot(aes(
            factor(clusters), 
            as.numeric(value),
            group = name)
        ) +
        geom_line(color = swire_colors$red) +
        # geom_line(aes(color = name), group = 1) +
        geom_smooth(aes(group = name), se = FALSE) +
        labs(
            title = title
            , subtitle = "Two clustering algorithm methods: kmeans and hierarchical"
        ) +
        facet_wrap(~name, ncol = 1, scales = "free") +
        coord_fixed() +
        theme_swire()
}
```

#### Clustering Exploration

```{r}
clustering_grid_results_sampl_70 <-
    clustering_grid |>
    mutate(
        kmeans = map(clusters, ~ cluster_kmeans(.x, swire_pca_70_sampl))
        ,hclust = map(clusters, ~ cluster_hclust(.x, swire_pca_70_sampl))
    )
```

```{r}
show_cluster_results(
    clustering_grid_results_sampl_70
    , "Cluster Results | 70% PCA, 5K Sample"
)
```

This graph gives us a sense for the "elbow point", the crux where we get diminishing returns in WSS from splitting into more and more clusters.

That looks to be about 5 (CONFIRM). We could double check that pretty easily by running the above functions again but for all of the data:

```{r eval = FALSE}
clustering_grid_results_70 <-
    clustering_grid |>
    mutate(
        kmeans = map(clusters, ~ cluster_kmeans(.x, swire_pca_70))
        ,hclust = map(clusters, ~ cluster_hclust(.x, swire_pca_70))
    )
```

```{r eval = FALSE}
show_cluster_results(
    clustering_grid_results_70
    , "Cluster Results | 70% PCA, All Data"
)
```

Let's generate cluster assignments!


#### Cluster Assignments

```{r}
fit <- kmeans(swire_pca_70, 5)

swire_cust_clustered <- swire_cust_enriched |> mutate(cluster = factor(fit$cluster))
```


#### Cluster Evaluation

Now, let's evaluate the results, starting with how many customers are in each cluster:

```{r}
swire_cust_clustered |>
  group_by(cluster) |>
  count()
```

Other things to explore...


#### Naming Clusters

A big part of clustering is learning about what is driving the assignment. In so doing, we'll gain insight about the fields that best describe the cluster. Those insights will help us name the clusters, bringing them to life in a powerful way for our analysis.





### Approach N

Lorem ipsum...


## Modeling Results

### Insight 1

Lorem ipsum...


### Insight 2

Lorem ipsum...


### Insight 3

Lorem ipsum...


## Future Improvements

Lorem ipsum...


## Conclusion

Lorem ipsum...


## Division of Work

Formulation of this group modeling assignment was made possible thanks to the collaborative partnership from all group members. 

As a group, we've partnered in the following ways:

orem ipsum...


Each member contributed with individual work as described below:

Lorem ipsum...


Thank you,

Adam, Georgia, Tyler, Zac